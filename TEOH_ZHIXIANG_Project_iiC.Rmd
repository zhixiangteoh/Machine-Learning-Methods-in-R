---
title: "CS 1675 Final Project"
subtitle: "Part iiC Regression models"
author: "Teoh Zhixiang"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages and data

```{r, load_packages}
library(tidyverse)
```


```{r, load_data}
df <- readr::read_rds("df.rds")
step_1_df <- readr::read_rds("step_1_df.rds")
step_2_a_df <- readr::read_rds("step_2_a_df.rds")
step_2_b_df <- readr::read_rds("step_2_b_df.rds")
mod_lm_basis <- readr::read_rds("mod_lm_basis.rds")
mod_lm_step1 <- readr::read_rds("mod_lm_step1.rds")
```

## Overview

i. Exploration
ii. **Regression models**
iii. Binary classification option b
iv. Binary classification option a
v. Interpretation and optimization

This R Markdown file tackles part iiC, specifically training, evaluating, tuning, and comparing models for `response_1` to the step 1 inputs, using more complex methods, via resampling.

iiA. Regression models - `lm()`
iiB. Regression models - Bayesian
**iiC. Regression models - Models with Resampling**

We will use the `caret` package to handle training, testing, and evaluation.

```{r, load_caret, eval=TRUE}
library(caret)
```

## General

Throughout all methods, we will use 5-fold cross validation, as our resampling method, by specifying `"repeatedcv"` as the `method` argument to a `caret::trainControl()`. Similar to before, we will use `Rsquared` as our performance metric.

From here on out, we will use `basis_form` to refer to our previously defined basis formula.

```{r, general_settings}
my_ctrl <- caret::trainControl(method = "repeatedcv", number = 5, repeats = 5)
rsquared_metric <- "Rsquared"
basis_form <- as.formula(paste("response_1 ~ xA + xB*(splines::ns(x01, df = 2) + splines::ns(x02, df = 2) + splines::ns(x03, df = 2))"))
```

## Linear additive model

First we will train a linear additive model on our previously defined basis function using `caret::train` and setting `method="lm"`. 

The main purpose of this linear additive model is to provide a baseline comparison to the other complex models we will train.

```{r, linear_additive_model}
set.seed(12345)
mod_lin_add <- caret::train(form = basis_form,
                            method = "lm", 
                            metric = rsquared_metric, 
                            trControl = my_ctrl,
                            data = step_1_df)

mod_lin_add
```

## Regularized regression with elastic net

We will train two different models with interactions, specifically one with all pair interactions between all step 1 input variables, and one with all 
triplet interactions. Given $n$ coefficients, the number of coefficients that must be learned for a model with all $m$-input interactions will be modeled by:

$$
\sum_{k=1}^{m} \left( \begin{array} {c} n \\ k \end{array} \right)
$$

**However, for each categorical input variable, we have to subtract the number of possibilites of interactions between categories within a single variable.**

So, for all pair interactions between all step 1 continuous input variables, the number of coefficients that must be learned is $10 + \left(\begin{array}{c} 10 \\ 2 \end{array}\right) - \left(\begin{array}{c} 3 \\ 2 \end{array}\right) = 52$, excluding intercept. For all triplet interactions, the number of coefficients that must be learned is $52 + \left(\begin{array}{c} 10 \\ 3 \end{array}\right) - 3\left(\begin{array}{c} 10 - 3 \\ 1 \end{array}\right) - \left(\begin{array}{c} 3 \\ 3 \end{array}\right) = 150$, excluding intercept.

### All pair interactions between step 1 inputs

Let's first fit a regularized regression model with elastic net, on all pairwise interactions between all step 1 inputs, using `caret::train` with `method="glmnet"`. We specify centering and scaling as preprocessing steps.

```{r, mod_glmnet_2}
set.seed(12345)
mod_glmnet_2 <- caret::train(response_1 ~ (.)^2,
                             method = "glmnet",
                             preProcess = c("center", "scale"),
                             metric = rsquared_metric,
                             trControl = my_ctrl,
                             data = step_1_df)

mod_glmnet_2
```

Check number of coefficients:

```{r, mod_glmnet_2_check_coef}
mod_glmnet_2$coefnames %>% length()
(model.matrix(response_1 ~ (.)^2, data = step_1_df) %>% colnames() %>% length() - 1) - (mod_glmnet_2$coefnames %>% length())
```

Visualize trends of metric Rsquared with respect to mixing percentage `alpha` and regularization parameter `lambda`.

```{r, mod_glmnet_2_ggplot}
ggplot(mod_glmnet_2)
```

Create a custom tuning grid `enet_grid` to try out many possible values of the penalty factor (`lambda`) and the mixing fraction (`alpha`).

```{r, enet_grid}
enet_grid <- expand.grid(alpha = seq(0.1, 0.9, by = 0.1),
                         lambda = exp(seq(-6, 0.5, length.out = 25)))
```

Now retrain the pairwise interactions model using `tuneGrid = enet_grid`, then extracting the optimal tuning parameters.

```{r, mod_glmnet_2_b}
set.seed(12345)
mod_glmnet_2_b <- caret::train(response_1 ~ (.)^2,
                               method = "glmnet",
                               preProcess = c("center", "scale"),
                               tuneGrid = enet_grid,
                               metric = rsquared_metric,
                               trControl = my_ctrl,
                               data = step_1_df)

mod_glmnet_2_b$bestTune
```

Print out the non-zero coefficients, specifying the optimal value of lambda identified by resampling.

```{r}
coef(mod_glmnet_2_b$finalModel, s = mod_glmnet_2_b$bestTune$lambda) %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column("coef_name") %>% 
  tibble::as_tibble() %>% 
  purrr::set_names(c("coef_name", "coef_value")) %>%
  filter(coef_value != 0)
```

Visualize trends of metric Rsquared with respect to mixing percentage `alpha` and regularization parameter `lambda`, for model trained with our defined `enet_grid`.

```{r, mod_glmnet_2_b_plot}
plot(mod_glmnet_2_b, xTrans = log)
```

### All triplet interactions between all step 1 inputs

Now fit a regularized regression model with elastic net, on all triplet interactions between all step 1 inputs, using `tuneGrid = enet_grid`, then displaying the optimal tuning parameters.

```{r, mod_glmnet_3_b}
set.seed(12345)
mod_glmnet_3_b <- caret::train(response_1 ~ (.)^3,
                               method = "glmnet",
                               preProcess = c("center", "scale"),
                               tuneGrid = enet_grid,
                               metric = rsquared_metric,
                               trControl = my_ctrl,
                               data = step_1_df)

mod_glmnet_3_b$bestTune
```

Check number of coefficients:

```{r, mod_glmnet_3_b_check_coef}
mod_glmnet_3_b$coefnames %>% length()
(model.matrix(response_1 ~ (.)^3, data = step_1_df) %>% colnames() %>% length() - 1) - (mod_glmnet_3_b$coefnames %>% length())
```

Visualize trends of metric Rsquared with respect to mixing percentage `alpha` and regularization parameter `lambda`, for model trained with our defined `enet_grid`.

```{r, mod_glmnet_3_b_plot}
plot(mod_glmnet_3_b, xTrans = log)
```

### Basis function

Now fit a regularized regression model with elastic net, on basis function previously defined, using `tuneGrid = enet_grid`, then displaying the optimal tuning parameters.

```{r, mod_glmnet_basis}
set.seed(12345)
mod_glmnet_basis <- caret::train(basis_form,
                                 method = "glmnet",
                                 preProcess = c("center", "scale"),
                                 tuneGrid = enet_grid,
                                 metric = rsquared_metric,
                                 trControl = my_ctrl,
                                 data = step_1_df)

mod_glmnet_basis$bestTune
```

Check number of coefficients:

```{r, mod_glmnet_basis_check_coef}
mod_glmnet_basis$coefnames %>% length()
(model.matrix(basis_form, data = step_1_df) %>% colnames() %>% length() - 1) - (mod_glmnet_basis$coefnames %>% length())
```

Visualize trends of metric Rsquared with respect to mixing percentage `alpha` and regularization parameter `lambda`, for model trained with our defined `enet_grid`.

```{r, mod_glmnet_basis_plot}
plot(mod_glmnet_basis, xTrans = log)
```

### Compare

Compare resampling results across the three different models.

```{r, glmnet_compare}
glmnet_results <- resamples(list(glmnet_2way = mod_glmnet_2_b,
                                 glmnet_3way = mod_glmnet_3_b,
                                 glmnet_basis = mod_glmnet_basis))

dotplot(glmnet_results)
```

`glmnet_basis` clearly is the best model, followed by `glmnet_3way`.

```{r, mod_glmnet_basis_results}
mod_glmnet_basis$results %>% filter(alpha == mod_glmnet_basis$bestTune$alpha & lambda == mod_glmnet_basis$bestTune$lambda)
```

## Neural network

### Single hidden layer

Fit a neural network regression model, specified by setting `linout = TRUE`. First use the default tuning parameter search grid.

```{r, mod_nnet}
# n <- length(mod_lm_basis$coefficients)
# num_hidden_units <- round(2/3 * n)
set.seed(12345)
mod_nnet <- caret::train(form = basis_form,
                         method = "nnet",
                         preProcess = c("center", "scale"),
                         linout = TRUE,
                         trace = FALSE,
                         metric = rsquared_metric,
                         trControl = my_ctrl,
                         data = step_1_df)

mod_nnet
```

Optimal tuning parameters found by `caret`'s `method="nnet"` are 5 hidden units (single layer), 0.1 decay.

Now using a more refined search grid over the tuning parameters.

```{r, nnet_grid}
nnet_grid <- expand.grid(size = c(2, 4, 6, 8, 10),
                         decay = c(1e-4, 0.1, 0.5))
```

Fitting the `nnet` model using this more refined search grid. The grid search might take a few minutes.

```{r, mod_nnet_b}
set.seed(12345)
mod_nnet_b <- caret::train(form = basis_form,
                           method = "nnet",
                           tuneGrid = nnet_grid,
                           preProcess = c("center", "scale"),
                           linout = TRUE,
                           trace = FALSE,
                           metric = rsquared_metric,
                           trControl = my_ctrl,
                           data = step_1_df)

mod_nnet_b
```

It is observed that the neural network model favors a higher number of hidden units in a single layer.

#### All step 1 inputs

Fit a neural network model to all step 1 inputs, using our defined `nnet_grid`.

```{r, mod_nnet_step1}
set.seed(12345)
mod_nnet_step1 <- caret::train(response_1 ~ .,
                               method = "nnet",
                               tuneGrid = nnet_grid,
                               preProcess = c("center", "scale"),
                               linout = TRUE,
                               trace = FALSE,
                               metric = rsquared_metric,
                               trControl = my_ctrl,
                               data = step_1_df)

mod_nnet_step1
```

## Random forest

Random forests have become a handy and convenient learning algorithm that has good predictive performance with "relatively little hyperparameter tuning". We will use `method = "rf"` that allows us to use `caret::train` as we have for all other models. Importantly, we set `importance = TRUE` (no pun intended). The code chunk below might take a few minutes to run to completion.

```{r, mod_rf_step1}
set.seed(12345)
mod_rf_step1 <- caret::train(response_1 ~ .,
                             method = "rf",
                             importance = TRUE,
                             trControl = my_ctrl,
                             metric = rsquared_metric,
                             data = step_1_df)

mod_rf_step1
```

Fit the `rf` model to the basis function.

```{r, mod_rf_basis}
set.seed(12345)
mod_rf_basis <- caret::train(basis_form,
                             method = "rf",
                             importance = TRUE,
                             trControl = my_ctrl,
                             metric = rsquared_metric,
                             data = step_1_df)

mod_rf_basis
```

## Gradient boosted tree

Gradient boosting machines (GBM) build shallow trees in sequence, with each tree "learning and improving on the previous one"; as opposed to random forests which build deep independent trees. When gradient boosted and tuned, these shallow trees collectively form one of the best predictive models.

Set `method = "xgbTree"` in `caret::train`.

```{r, mod_gbm_step1}
set.seed(12345)
mod_gbm_step1 <- caret::train(response_1 ~ .,
                              method = "xgbTree",
                              verbose = FALSE,
                              metric = rsquared_metric,
                              trControl = my_ctrl,
                              data = step_1_df)

mod_gbm_step1
```

The best model identified has 150 iterations (`n.trees`), complexity (`interaction.depth`) of 3, learning rate (`shrinkage`) of 0.1, and minimum number of training set samples in a node to commence sampling (`m.minobsinnode`) of 10.

Fit the `xgbTree` model to basis function.

```{r, mod_gbm_basis}
set.seed(12345)
mod_gbm_basis <- caret::train(basis_form,
                              method = "xgbTree",
                              verbose = FALSE,
                              metric = rsquared_metric,
                              trControl = my_ctrl,
                              data = step_1_df)

mod_gbm_basis
```

## Support Vector Regression

The motivation for fitting a Support Vector Machine (SVM) is that SVMs have several advantages compared to other methods, as mentioned in the Hands-On Machine Learning with R book:

- SVMs attempt to maximize generalizability;
- SVMs are always guaranteed to find a global optimum since they are convex optimization problems, and don't get stuck in local optima unlike in neural networks;
- SVMs are relatively robust to outliers by using a cost parameter;
- SVMs are flexible in adapting to complex nonlinear decision boundaries using kernel functions, e.g. the radial basis kernel

First, load `kernlab` library.

```{r, load_kernlab}
library(kernlab)
```

In Support Vector Regression (SVR), we will stick to the general rule of thumb to use a radial basis kernel in our `caret::train` call, using `method="svmRadial"`.

First see what are the parameters to be learned:

```{r, getModelInfo_svmRadial}
caret::getModelInfo("svmRadial")$svmRadial$parameters
```

Now fit the model.

```{r, mod_svr}
set.seed(12345)
mod_svr <- caret::train(form = basis_form,
                        method = "svmRadial",
                        preProcess = c("center", "scale"),
                        tuneLength = 10,
                        metric = rsquared_metric,
                        trControl = my_ctrl,
                        data = step_1_df)

mod_svr
```

Plot the results to see cross-validated `Rsquared` scores against different cost values.

```{r, mod_svr_plot}
ggplot(mod_svr)
```

As suggested by the model training outcome, and the plot above, `C = 8` provides the best cross-validated `Rsquared` scores. Checking the results:

```{r, mod_svr_results}
mod_svr$results

mod_svr$results %>% filter(C == 8)
```

It is clear that the model with `C = 8` is the best across most metrics, with the exception of `MAE`, and `Rsquared` standard deviation.

## Multivariate Adaptive Regression Splines

The motivation for fitting a Multivariate Adaptive Regression Splines (MARS) model is to explore more nonlinear relationships between the step 1 inputs, unlike the first linear three models we trained in part iiA. Although our previously defined basis function that crosses categorical inputs with natural spline functions of `x01` to `x03` has been by far the best model, MARS is capable of extending linear models to capture multiple nonlinear relationships by searching for and discovering nonlinearities and interactions in the data that will help maximize predictive accuracy.

First, load `earth` library for MARS modeling.

```{r, load_earth}
library(earth)
```

Hands-On Maching Learning in R describes the inner workings of MARS. Instead of explicitly defining polynomial functions or natural spline functions ourselves, MARS provides a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints, like step functions. The procedure assesses each data point for each input as a knot and creates a linear regression model with the candidate feature(s). 

### Basic MARS model

```{r, mod_mars_basic}
mod_mars_basic <- earth(response_1 ~ ., 
                        data = step_1_df)

mod_mars_basic %>% summary()
```

This basic model tells us that 7 out of the 10 predictors were used, leaving out `xA`, `x04` and `x05`, with a resulting `Rsquared` of `0.5406884`.

```{r, mod_mars_basic_plot}
plot(mod_mars_basic)
```

### Tuning and resampling

To help in the tuning of this procedure, we can specify tuning parameters such as the maximum degree of interactions, `degree`, and the number of terms retained in the final model, `nprune`, in a tuning grid to be passed into the `caret::train` call. Since there is rarely any benefit in assessing greater than triplet interactions, we choose `degree = 1:3`. We also start out with 10 evenly spaced values and intend to zoom in when we later find an approximate optimal solution and there is cause to.

```{r, mars_grid}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = seq(2, 100, length.out = 10) %>% floor())

mars_grid %>% head()
```

We will use `caret::train`, as in the previous sections. The grid search might take a few minutes.

```{r, mod_mars}
set.seed(12345)
mod_mars <- caret::train(response_1 ~ .,
                         method = "earth",
                         tuneGrid = mars_grid,
                         metric = rsquared_metric,
                         trControl = my_ctrl,
                         data = step_1_df)

mod_mars$bestTune

mod_mars
```

The best model identified has `Rsquared` value of `0.8148037`, with parameters `nprune = 23` and `degree = 2`.

Plot the model.

```{r, mod_mars_plot}
ggplot(mod_mars)
```

For both `degree = 2` and `degree = 3`, they yield the same `Rsquared` values, explaining the overlap in the plot, and the "absence" of degree 2.

Here, because the optimal `Rsquared` values stay constant beyond roughly `nprune = 23` terms, there is no need to adjust to a more specific tuning grid.

```{r, mod_mars_resample_stats}
mod_mars$resample %>% summary()
```

## Comparing methods

Now that we have fit all of the models, we can compare the cross-validation hold-out set performance metrics. We first compile all of the “resample” results together with the resamples() function.

```{r, all_reg_mods_resample}
iiC_results <- resamples(list(lm_basis = mod_lin_add,
                              glmnet_basis = mod_glmnet_basis,
                              glmnet_2way = mod_glmnet_2_b,
                              glmnet_3way = mod_glmnet_3_b,
                              nnet_basis = mod_nnet_b,
                              nnet_step1 = mod_nnet_step1,
                              rf_basis = mod_rf_basis,
                              rf_step1 = mod_rf_step1,
                              gbm_basis = mod_gbm_basis,
                              gbm_step1 = mod_gbm_step1,
                              svr_basis = mod_svr,
                              mars_basis = mod_mars))
```

Then we visually compare the performance metrics.

```{r, iiC_results_dotplot}
dotplot(iiC_results)

dotplot(iiC_results, metric = "RMSE")
dotplot(iiC_results, metric = "Rsquared")
dotplot(iiC_results, metric = "MAE")
```

Manually extract the results to make a custom figure.

```{r, iiC_results_ggplot}
iiC_results_lf <- 
  as.data.frame(iiC_results, metric = "RMSE") %>% tibble::as_tibble() %>% 
    mutate(metric_name = "RMSE") %>% 
  bind_rows(as.data.frame(iiC_results, metric = "Rsquared") %>% tibble::as_tibble() %>% 
    mutate(metric_name = "Rsquared")) %>% 
  bind_rows(as.data.frame(iiC_results, metric = "MAE") %>% tibble::as_tibble() %>% 
    mutate(metric_name = "MAE")) %>% 
  tidyr::gather(key = "model_name", value = "metric_value", -Resample, -metric_name)

iiC_results_lf %>% 
  ggplot(mapping = aes(x = model_name, y = metric_value)) +
  geom_boxplot() +
  stat_summary(fun.data = "mean_se",
               color = "red",
               fun.args = list(mult = 1)) +
  coord_flip() +
  facet_grid(. ~ metric_name, scales = "free_x") +
  theme_bw()
```

Clearly, the three best models are `lm`, `glmnet_basis`, and `svr_basis`, in that order.

```{r,iiC_results_corr}
as.data.frame(iiC_results, metric = "RMSE") %>% 
  GGally::ggpairs(columns = 1:12)
```

Additionally, models `lm_basis` and `glmnet_basis` are highly correlated, as are `glmnet_2way`-`glmnet_3way`, `rf_basis`-`rf_step1`, `rf_step1`-`gbm_basis`.

## Saving

```{r, save_data}
mod_lin_add %>% readr::write_rds("mod_lin_add.rds")
mod_glmnet_basis %>% readr::write_rds("mod_glmnet_basis.rds")
mod_svr %>% readr::write_rds("mod_svr.rds")
```
